<!DOCTYPE html>

<!-- LOAD THE MATHJAX LIBRARY -->
<!-- DOCS: https://docs.mathjax.org/en/latest/basic/mathematics.html -->
<!-- DEMO: https://github.com/mathjax/MathJax-demos-web -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<html lang="en">
<head>
	<title></title>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/hmenu.css') }}">
  <link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/text.css') }}">
  <link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/footer.css') }}">
</head>
<body>
  <link href='https://fonts.googleapis.com/css?family=Roboto:500,900,100,300,700,400' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
		<header>
	    <nav class="stroke">
	      <ul>
	        <li><a href="../index.html">Home</a></li>
	        <li><a href="#">More</a></li>
	        <li><a href="#">More</a></li>
	        <li><a href="#">More</a></li>
	        <li><a href="#">About</a></li>
	      </ul>
	    </nav>
		</header>
	  <main>
	    <h1>CONVOLUTION vs. CROSS-CORRELATION</h1>
			<h2>On two often mixed up terms.</h2>
	    <div class=blog_text_container>

				<h3>Problem Definition</h3>
	      <p class=text>
	        The first step to a profound understanding of
	        <a href = "html/convnet.html" target = "_self"> Convolutional Neural Networks </a> (short ConvNets or CNNs)
	        is to grasp what these networks have to do with <i> Convolutions </i>, and why they
					actually havn't to do anything with it, e.g. in the majority of articles out there animations, showing
					the supposedly workflow of convolution for images is actually showing cross-correlation.
					This blog is aiming to outline both terms distinctively, such that you will not longer mix them up.
					Further at the end, you will find a neat introduction to ConvNets.
					Thrilled? Good, let's get started by buckle down on the term convolution.
				</p>

				<h3>Convolution</h3>
				<p class=text>
					When introducing terms like this one by beginning with the history and the inventer is a prevailing
					startegy in many publications. This invention, however, is hard to pinpoint for the notion of
					convolution, since the entire field of signal processing is reliant on this basic operation.
					So this actually brings us to one important point for the difference between convolution and
					cross-correltation, while signal processing is the main application for convolution, image processing,
					as we will see later, is the main application for cross-correlation.
				</p>
				<br>
 				<p class=text>
					As the name suggests, signal processing is the art of juggling with signals, which are mathematical
					functions, that convey information. For example, you think of information as the amount of voltage at a
					definite timestamp \( t \). Image a WiFi signal os transmitted from your router to your smartphone, which you
					might already do, when reading this artical on your phone connected to our home WiFi. Now,
					your smartphone is in this case the receiver, and when you just compare the originally transmitted signal
					and the signal received, you will unfortunately notice, that the received signal is subject
					to some kind of noise lying on top of the signal, modifying the broadcasted information slightly
					(remember thats the time-dependet voltage level in this case). Even though, I used the word slightly here,
					the impact is always relative, hence it depends on how fine-grainded the signal is,
					the quality of the underlying transmitter-receiver system, the distance, and more.
					An example is depicted in Figure 1. One can imagine, that such
					a conveyed signal needs some smoothing to be handled without loss of information or corruption of
					information.
				</p>

				<iframe
				src="/dashapp_noise/"
				class="dash_frame"
				height="550px">
				</iframe>
				<figcaption style="top: -40px;">
					<b>Fig.1</b> This shows <i>(a)</i> the vanilla signal,
					<i>(b)</i> the white (random) noise, and <i>(c)</i> the noisy signal.
				</figcaption>

				<p class=text>
					This can be tackled by the interconnection of a filter element after receiving a signal. With this we are
					diving into the highly interesting topic of digital signal processing (DSP). This can be <i> inter alia </i>
					be done by a convolutional filter element, this is mostly part of a digital signal processor (unfortunately, also
					often abbreviated by DSP). This signal smoothing can be achieved by a powerful magic weapon called
					convolution. Its a mathematical operation, which merges two signals, in our case the noisy input signal
					and a filter function, yielding a new signal, which constitues the modification of the original signal
					by the filter. This is not done by a simple point-wise addition of signal and filter values at
					each time step, instead for each time step the filter adds all signal values at this very time step
					and all previous steps, scaled by the filter value at the time step, together. Okay, this needs some math
					to be looked into. Lets consider the <b>Discrete Convolution</b> first, which is perhaps easier to gasp at the moment.
					Formally, this is defined as

					$$ f[t] * g[t] = \sum_{k=-\infty}^{\infty} f[k] g[t-k],$$

					where \( f[t] \) is our signal and \( g[t] \) is the filter (often also called kernel).
					The  \( * \) symbol indicates convolution between the two functions (often also represented by a
					\( \otimes \) or \( \bigstar \) ). Often the first questions are popping up regarding the \( n - k \) term.
					This is the so called <b> Time Shift </b>. Image we have given the following discrete signal depicted in Fig. 1 (top).
					If we now shift this signal to the right side, with other words move it along the time axis (like when time progresses),
					we just take the signal value at time step \( t = 2 \) for example and assign this value to the next time step (\( t = 3 \) in this case).
					Another way to think of it, is from the perspective of the shifted signal \( x[t-1] \): Each value is seized
					from the original signal \( x[t] \) but one value but one value before the current time step \( t \), hence the minus one.
				</p>

				<figure>
				 <img src="static/figures/articles/convolution/time_shift.png"
				 alt="Time Shift" style="width:70%">
				 <figcaption style="top: 0px">
					 <b>Fig.2</b> Time Shift example.
				 </figcaption>
				</figure>

				<p class=text>
					Let's have another peek into the discrete convolution formulation.
					Intuitively, you can think of this process like moving the signal over the filter.
					The filter is stays at its place and we iterate over each value  \( f[k] \).
					The signal on the other hand is shifted by the value of  \( t \), since \( t \) is fixed and \( k \) is variable, we say that the function
					is shifted by \( t \) not by \( k \). To introduce even more unfamiliarity, its value of \( k \) is negative, thus the signal
					is reflected over the vertival line at \( t=0 \). So naturally you will ask: Why should we reverse the signal?
					This comes from the most common use case of convolution, which is in the field of <b> Linear Time Invariant (LTI) Systems </b>.
					These systems are a worth a article for themselfes, so lets put it as simple as possible: For now lets just say that they
					produce a responce to an input signal. For example, when you hit a bell. The hit would look like a sudden peak, which is quickly
					decreasing until slowly the value decreases. You now want to convolve the system function with this hit function, and since you
					want to start at the point where you hit the bell, you need to reverse the hit curve. If this does not matter for your application,
					the reflection is in some sense arbitrary.
					When we peek back to the formular once again, we see that the convolution is time-dependend, thus its a function, and we can
					compute a value for each time step \( t \), formally \( f[t] * g[t] \rightarrow c[t] \). Note also that this whole thing
					follows commutativity, hence it doesn't matter if we shift the signal and set the filter fixed or vice versa, mathematically speaking:

					$$ f[t] * g[t] = \sum_{k=-\infty}^{\infty} f[k] g[t-k] = \sum_{k=-\infty}^{\infty} f[t-k] g[k]. $$

					Upto this point we merely accounted for discrete functions, but what if we want to describe continuous ones?
					This is called <b>Continous Convolution</b>, which is just a replacement of the sum operation by an integral
					and the exact same intuition applies here:

					$$ (f * g)(t) \triangleq \int_{-\infty}^{+\infty} f(\tau) g(t - \tau) d \tau, $$

					This is engaging from a theoretical standpoint, since in practice we descide for an appropriate computational accuracy
					and then approximate a continuous signal by discretizing it.
					Returning to our noise example, we can now apply a filter via convolution to the noisy signal depicted in Figure 2
					and hence smooth it to suppress the noise impact.
					<!-- For this application a <b>Low-Pass Filter</b> is the most inuitive choise (Speaking about commen filters, this is,
					again, worth an entire article on its own.).  -->
				</p>
				<br>
				<p class=text>
					For this purpose we need a appropriate filter, which smoothes the noisy signal. A proper choice would be the
					<b>Hamming Filter</b>, which is depicted in Figure 3a. In this article we will not immerse into the field of
					<b>Filter Design</b>, which is a extensive toptic on its own. Lets just say that the filter of choice is a
					well-known filter to use for such purposes. Obviously, convolving the signal and filter in a straightforward
					fashion wouldn't be much beneficial, since the filter would alter the shape of the signal completely. Whereas,
					when we apply the filter only locally for small patches of the signal its shape will not alter the global
					information of the signal, instead smooth region by region. The result is depicted in Fig. 3b.
				</p>

				<iframe
				src="/dashapp_conv/"
				class="dash_frame"
				height="470px">
				</iframe>
				<figcaption style="top: -40px;">
					<b>Fig.3</b> <i>(a)</i> The Hamming filer and
					<i>(b)</i> the convolved noisy signal and the filter to smooth it.
				</figcaption>

				<p class=text>
					One interesting property of convolution in the discrete, as well as in the continuous domain is the
					commutativity. Mathematically,

					$$ f(t) \otimes g(t) = g(t) \otimes f(t). $$

					Hence we can easily rearrange terms in more complex settings.
				</p>

				<button type="button" class="collapsible">Proof</button>
				<div class="content">
					<p class=collapse>
						To proof that, we can simply substitute \( t - \tau = \tau^\prime \), such that

						$$ f(t) \otimes g(t) = - \int_{+\infty}^{-\infty} f(t - \tau^\prime) g(\tau^\prime) d \tau^\prime. $$

						In other words, we are replacing \( \tau \) with \( t - \tau^\prime \), \( t - \tau \) with \( \tau^\prime \),
						and if there would be a single \( t \) it would get replaced by \( \tau^\prime - \tau \), just to complete
						the picture.
						Further, \( d \tau \) got replaced by \( - d \tau^\prime \), since

						$$ \frac{d\tau^\prime}{d\tau} = \frac{d(t - \tau)}{d\tau} \rightarrow d \tau^\prime = - d \tau. $$

						The attentive reader probably also recognized that the limits of the integral have changed, which is due to
						the reverved sign of the integration. However, since the limits are infinte, the origin shift introduced by the
						substitution by vector \( t \) does not alter the magnitude of the limits.
						Hence, we can reverse the order of the limits:

						$$ f(t) \otimes g(t) = \int_{-\infty}^{+\infty} f(t - \tau^\prime) g(\tau^\prime) d \tau^\prime. $$

						Now, we can swap \( f \) and \( g \) and replace \( \tau^\prime \) by \( \tau \), since this is just a
						arbitrary variable name, and we will finally end up with:

						$$ f(t) \otimes g(t) = \int_{-\infty}^{+\infty} g(\tau) f(t - \tau) d \tau $$
					</p>
				</div>
				<br>
				<p class=text>
					Clearly, the operation of convolution is not restricted to the 1D example above, it can be applied for
					multi-dimensional applications in just the same manner. This form of convolution is called
					<b>Multi-Dimentional (Discrete) Convolution</b>. Generically, we just loop over all dimensions,
					such that for the 2D case the convolution is defined by

					$$ f[w, h] ** g[w, h] = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} f[i, j] g[w-i, h-j], $$

					where \( ** \) denotes the 2D convolution. Indeed, by convention the number of asteriks is equals the
					number of dimensions.
					Our input is an image with width \( w \) and height \( h \).
					Since an image is just a grid of pixels, the kernel
					also follows grid-based structure. Remember that each image is just a
					arrangement of pixels, where each pixel is a set of scalar values. In case of RGB
					images each pixel has 3 values: one scalar value for Red, Green, and Blue.
					The kernel is a 3 by 3 patch, where each element (small squares
					in the in Figure 4) is just a scalar value of any value.
				</p>

				<figure>
				 <img src="static/figures/articles/convolution/conv.gif"
				 alt="Convolution Animation" style="width:35%">
				 <figcaption style="top: 0px">
					 <b>Fig.4</b> Applying a kernel to an image.
				 </figcaption>
				</figure>

				<p class=text>
						This toolkit is necessary for image processing.
						All the majority of simply image filters you know from your image editing software,
						are multidimensional discrete kernel functions, which are applied
						to your image at hand to produce a gaussian blur or a crisp sharpening
						satifying your needs.
						Let us use this image as a starting point, see Figure 5.
				</p>

				<figure>
				 <img src="static/figures/articles/convolution/raccoon.jpg"
				 alt="Raccoon Original" style="width:70%">
				 <figcaption style="top: 0px">
					 <b>Fig.5</b> Vanilla example image taken from
					 <a href = "https://unsplash.com" target = "_self">Unsplash</a>.
				 </figcaption>
				</figure>

				<p class=text>
						Since this is a high-definition image, let us rescale it to 256x256
						pixels to cut down on computational cost and for the visual appearance
						as you will see later. To further simplify this, lets only consider one
						color channel. The original image follows a RGB color encoding, which
						comprises a channel for the red (R), green (G), and blue (B) pixel values.
						Reducing this to one color channel, means getting rid of the colors,
						and render the image to a greyscale picture.
						You can see the resulting image in Figure 4.
				</p>

				<figure id=“twoimages”>
				 <img src="static/figures/articles/convolution/raccoon_resized.jpeg"
				 alt="Raccoon Rescaled">
				 <img src="static/figures/articles/convolution/raccoon_resized_grey.jpeg"
				 alt="Raccoon Grey">
				 <figcaption style="top: 0px">
					 <b>Fig.5</b> (left) Image rescaled to 256x256x3 pixels,
					 (right) Image as greyscale 256x256x1 pixels.
				 </figcaption>
				</figure>

				<p class=text>
						Fig. 6 lets you interactivelly play around with the kernel position
						and the resulting convoled output pixel.
				</p>

				<figure>
					<canvas id="input_canvas"></canvas>
					<figcaption style="top: 0px">
						<b>Fig.6</b> Interactive 2D image convolution.
					</figcaption>
				</figure>

				<p class=text>
						Okay, lets hit pause at this point and think about the implications here.
						By changing the kernel, undoubtedly the output image would change, which
						totally makes sense. What if we would stack multiple kernels on top of each other
						and use the output of the first convolution as the input for the next one and so on.
						This sequential procedure could be used to first detect minor features, like edges,
						and further down the line more sophisticated ones, like eyes. In that way a
						image classificator can be build, able to label images in certain way.
						Designing the different kernels properly however would be an uphill battle,
						due to the vast amount of possiblities. Alternatively, we could losen the
						rigid kernel values and let a <b>Neural Network</b> learn them for us instead.
						These networks are called <b>Convolutional Neural Network</b>.
						In the <b>Machine Learning</b> commutity a neady gready discussion emerged in some forums
						about why these networks are called <b>Convolutional Neural Network</b>, rather than
						<b>Cross-Correlation Neural Network</b>. To participate, lets clarify the difference
						between convolution and cross-correlation.
				</p>

				<h3>Cross-Correlation</h3>
				<p class=text>
						Cross-Correlation is pretty similar to convolution, the only difference is that
						one don't need to flip the kernel. Formally speaking, this means

						$$ corr(f[t], g[t]) = \sum_{k=-\infty}^{\infty} f[k] g[t+k], $$

						hence this implies the following relationship

						$$ corr(f[t], g[t]) = f[t] * g[-t].$$

						Hence, the kernel is simply slided over the signal, with this we can
						quantify similarities between both functions. Indeed, in <b>Statistics</b>
						cross-correlation is used to identity correlations between two functions
						with one of which shifted in time. This directly leads to the presuming
						question of why is this for any analytical interest? Assume you have given
						you invest in a Startup and you invest into precise features. Since the features
						need to be developed first, you will see the results delayed. Fig. 7a depicts
						your investment and the purchases over time. Clearly, it is hard to spot any
						correlation at first, but when you shift the curve back in time (see Fig. 7b),
						you can actually spot the relationship at a glance. That is, the investment in
						a new feature pushed the purchases nearly every time. Note, we assume equal
						durations for all feature development phases.
				</p>

				<iframe
				src="/dashapp_corr/"
				class="dash_frame"
				height="470px">
				</iframe>
				<figcaption style="top: -40px;">
					<b>Fig.7</b> <i>(a)</i> The investments (blue) and the purchases (orange) and
					<i>(b)</i> the shifted purchases back in time.
				</figcaption>

				<p class=text>
						Therefore, when we shift the purchase-curve over the time axis and compute
						the sum over all values (in the discrete case) or the integral (in the
						continuous case) and hence the area under the combined curve, we will get the
						cross-correlation curve. The maximum of this curve will be the point where both
						signals correlate the most with each other, with other words the situation
						shown in Fig. 7b. Thus we can infer that the result w.r.t. the purchases of each
						investigated feature is delayed by this specific time value. 
				</p>

				<h3>Conclusion</h3>
				<p class=text>
						Ultimately, we can say that, since <i>ConvNets</i> possess the ability to
						learn their weights, they also possess the ability the flip their kernels.
						Therefore, critising the naming of these networks is justified but superfluous.
						Indeed, most <i>ConvNets</i>-based architectures employ cross-correlation under
						the hood, but still if you change this intrinsic operation of the network, it
						will adapt accordingly and learn flipping the weights. Due to <i>Occam's Razor</i>
						the simpler consistent hypothesis is the better one. Cross-Correlation is simpler
						in this case, since we save flipping the kernel, which otherwise constitutes an
						additional operation to <i>backpropagate</i> and to consider for <i>inference</i>.
						At this point, I hope, I could gave you an impression of use cases and differences
						between both of these strongly related methods.
				</p>

				<h3>References</h3>
				<p class=references>
						[1] [Blog] <a href = "https://glassboxmedicine.com/2019/07/26/convolution-vs-cross-correlation/" target = "_self">Glassboxmedicine</a>
				</p>

					<!-- However, this would inflate the article just too much.
					Nevertheless, this should give you an intuitive introduction to the notion of convolution.
					Lets stop here and reflect our initial motivation for a momenten. We motivated
					this topic as an introduction and base line for understanding the functionality of
					<a href = "html/convnet.html" target = "_self"> Convolutional Neural Networks </a>.
					These networks, however, function not based on convolution, instead they use a similar
					mathematical operation, called Cross-Correlation.

					Now, since we have get an impression of what lies hidden under the term
					convolution, lets have a brief look into the related field of correlation.
					Similar to convolution, in the process of correlation, we apply a kernel
					to some input. However, we have to be more specific here, since the
					applications of the correlation operation are limited to image processing only.
				</p> -->


				<p id="objectCoords"></p>

					<!-- first animation with discrete convolution (1D) and then
				 	extend it into 2D -->

					<!-- [1](https://www-structmed.cimr.cam.ac.uk/Course/Convolution/convolution.html) -->
					<!-- https://www.rbjlabs.com/signals/convolution-of-signals-continuous-and-discrete/ -->

				<footer>
					<div class="social">
					  <i class="fa fa-twitter" id="twitter"></i>
					  <i class="fa fa-facebook" id="facebook"></i>
					  <i class="fa fa-instagram" id="instagram"></i>
					  <i class="fa fa-whatsapp" id="whatsapp"></i>
					</div>

					<div class="copyright">
						Copyright &copy; <script>document.write(new Date().getFullYear())</script>, Johann Schmidt
					</div>
				</footer>
	  </main>

	<script src="static/js/footer_social_icons_animation.js"></script>
	<script src="static/js/collapsible_animation.js"></script>
	<script src="static/js/kernel_inspection.js"></script>

</body>
